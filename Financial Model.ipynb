{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25f48f3-65f3-4e54-a0ed-c79691185949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e813ce4-3952-44eb-85bb-a278690bc0ae",
   "metadata": {},
   "source": [
    "## Discription:\n",
    "\n",
    "This is my test code for my ETF rebate financial algorithm for a research project I did over the summer of 2024. The idea was to test it to see if my algorithm correctly modeled every daily rebate for one ETF. Then I was to test and see if it would work on two to make sure it could be applied to all ETFs. Once it passed those two tests, I was able to use Bridges 2 supercomputer to apply it to over 700 etfs at the same time. The idea was to be able to clean every data set simulatniously accounting for all possiblites to ensure correct calculations for all of the ETFs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24fdfb8-7c86-444c-a633-23da27297054",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e16188-c713-4523-82bc-cf0e9ca6a661",
   "metadata": {},
   "source": [
    "### Test 1:\n",
    "\n",
    "This algorithm leverages financial modeling techniques to process and analyze ETF, rebate, and federal funds rate (FFR) data. It dynamically imputes missing ticker symbols, ensuring data integrity, then merges the financial datasets based on date and matching identifiers to create a comprehensive view of ETF constituents. By filtering out high-weight positions and excluding non-relevant bond data, the model refines its dataset for more accurate analysis. It employs advanced data cleaning methods, including missing utilization imputation and multiple rebate calculations based on both 100% and actual utilization. The results are aggregated at the ETF and date level, producing actionable insights into daily rebates and their adjustments. The model outputs aggregated and merged datasets in CSV format, enabling deeper financial analysis and decision-making in portfolio management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33010378-270a-4028-9643-e339b3add7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fill_missing_tickers(etf_data):\n",
    "    missing_tickers = etf_data[etf_data['Constituent_Ticker'].isna()]\n",
    "    for index, row in missing_tickers.iterrows():\n",
    "        matching_rows = etf_data[(etf_data['Constituent_Name'] == row['Constituent_Name']) & (etf_data['Constituent_Ticker'].notna())]\n",
    "        if not matching_rows.empty:\n",
    "            etf_data.at[index, 'Constituent_Ticker'] = matching_rows.iloc[0]['Constituent_Ticker']\n",
    "    return etf_data\n",
    "\n",
    "def analyze_missing_utilization(etf_file, rebate_data_file, ffr_data_file):\n",
    "    # Read and process rebate data\n",
    "    rebate_data = pd.read_csv(rebate_data_file, parse_dates=['DataDate'], low_memory=False)\n",
    "    rebate_data['BB_TICKER'] = rebate_data['BB_TICKER'].str.upper().str.strip()\n",
    "    rebate_data['ISIN'] = rebate_data['ISIN'].str.upper().str.strip()\n",
    "    rebate_data['SEDOL'] = rebate_data['SEDOL'].str.upper().str.strip()\n",
    "    rebate_data['InstrumentName'] = rebate_data['InstrumentName'].str.upper().str.strip()\n",
    "\n",
    "    ffr_data = pd.read_csv(ffr_data_file, parse_dates=['DATE'], low_memory=False)\n",
    "    ffr_data.rename(columns={'DATE': 'DataDate', 'DFF': 'FedFundsRate'}, inplace=True)\n",
    "    ffr_data['FedFundsRate'] = ffr_data['FedFundsRate'].astype(float) * 100 / 365  # Convert FFR to bps per day\n",
    "\n",
    "    rebate_data = rebate_data.merge(ffr_data, on='DataDate', how='left')\n",
    "    \n",
    "    rebate_data['IndicativeRebate'] = rebate_data['IndicativeRebate'].abs() * 10000 / 360  # Take absolute value and convert to BPS\n",
    "\n",
    "    etf_data = pd.read_csv(etf_file, parse_dates=['As_Of_Date'], low_memory=False)\n",
    "    etf_data['Constituent_Ticker'] = etf_data['Constituent_Ticker'].str.upper().str.strip()\n",
    "    etf_data['ISIN'] = etf_data['ISIN'].str.upper().str.strip()\n",
    "    etf_data['SEDOL'] = etf_data['SEDOL'].str.upper().str.strip()\n",
    "    etf_data['Constituent_Name'] = etf_data['Constituent_Name'].str.upper().str.strip()\n",
    "\n",
    "    etf_data = fill_missing_tickers(etf_data)\n",
    "\n",
    "    etf_name = etf_file.split('_')[-1].split('.')[0]\n",
    "    etf_data['ETF'] = etf_name\n",
    "\n",
    "    # Filter out rows where 'Weight' exceeds 0.25\n",
    "    etf_data = etf_data[etf_data['Weight'] <= 0.25]\n",
    "\n",
    "    keys_to_try = [\n",
    "        ('Constituent_Ticker', 'BB_TICKER'),\n",
    "        ('Constituent_Name', 'InstrumentName'),\n",
    "        ('SEDOL', 'SEDOL'),\n",
    "        ('ISIN', 'ISIN')\n",
    "    ]\n",
    "\n",
    "    merged_data = pd.DataFrame()\n",
    "    for etf_key, rb_key in keys_to_try:\n",
    "        if etf_data[etf_key].isnull().all() or rebate_data[rb_key].isnull().all():\n",
    "            continue\n",
    "\n",
    "        temp_merged = etf_data.merge(rebate_data, left_on=['As_Of_Date', etf_key], right_on=['DataDate', rb_key], how='left', indicator=True)\n",
    "\n",
    "        valid_rows = temp_merged[(pd.notnull(temp_merged[etf_key])) & (pd.notnull(temp_merged[rb_key]))]\n",
    "\n",
    "        if not valid_rows.empty:\n",
    "            if merged_data.empty:\n",
    "                merged_data = valid_rows.copy()\n",
    "            else:\n",
    "                merged_data = pd.concat([merged_data, valid_rows])\n",
    "\n",
    "    if merged_data.empty:\n",
    "        print(f\"No valid rows after merging for ETF: {etf_name}\")\n",
    "        return\n",
    "\n",
    "    # Ensure 'InstrumentName' is a string before filtering\n",
    "    merged_data['InstrumentName'] = merged_data['InstrumentName'].astype(str)\n",
    "\n",
    "    # Exclude bonds by filtering out rows with '%' in InstrumentName\n",
    "    merged_data = merged_data[~merged_data['InstrumentName'].str.contains('%')]\n",
    "\n",
    "    # Drop duplicate rows for the same ETF-stock-day combination, keeping the first row only\n",
    "    merged_data = merged_data.drop_duplicates(subset=['As_Of_Date', 'Constituent_Ticker'], keep='first')\n",
    "\n",
    "    # Count missing utilization values\n",
    "    missing_util_count = merged_data['Utilisation'].isna().sum()\n",
    "    print(f\"ETF: {etf_name}, Missing Utilisation Count: {missing_util_count}\")\n",
    "\n",
    "    # Calculate Daily Rebate assuming 100% Utilisation\n",
    "    merged_data['Daily_Rebate_100_Util'] = merged_data['Weight'] * merged_data['IndicativeRebate']\n",
    "\n",
    "    # Calculate Adjusted Daily Rebate assuming 100% Utilisation\n",
    "    merged_data['Adjusted_Daily_Rebate_100_Util'] = merged_data['Weight'] * (merged_data['IndicativeRebate'] + merged_data['FedFundsRate'])\n",
    "\n",
    "    # Handle missing Utilisation by imputing with 0\n",
    "    merged_data['Utilisation'] = merged_data['Utilisation'] / 100\n",
    "\n",
    "    # Calculate Daily Rebate with actual Utilisation\n",
    "    merged_data['Daily_Rebate'] = merged_data['Weight'] * merged_data['IndicativeRebate'] * merged_data['Utilisation']\n",
    "\n",
    "    # Calculate Adjusted Daily Rebate with actual Utilisation\n",
    "    merged_data['Adjusted_Daily_Rebate'] = merged_data['Weight'] * (merged_data['IndicativeRebate'] + merged_data['FedFundsRate']) * merged_data['Utilisation']\n",
    "\n",
    "    # Ensure all values are in decimal format\n",
    "    merged_data['Daily_Rebate_100_Util'] = merged_data['Daily_Rebate_100_Util'].apply(lambda x: f\"{x:.10f}\")\n",
    "    merged_data['Adjusted_Daily_Rebate_100_Util'] = merged_data['Adjusted_Daily_Rebate_100_Util'].apply(lambda x: f\"{x:.10f}\")\n",
    "    merged_data['Daily_Rebate'] = merged_data['Daily_Rebate'].apply(lambda x: f\"{x:.10f}\")\n",
    "    merged_data['Adjusted_Daily_Rebate'] = merged_data['Adjusted_Daily_Rebate'].apply(lambda x: f\"{x:.10f}\")\n",
    "\n",
    "    daily_aggregated = merged_data.groupby(['As_Of_Date', 'ETF']).agg({\n",
    "        'Daily_Rebate': 'sum',\n",
    "        'Adjusted_Daily_Rebate': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    daily_aggregated_100_util = merged_data.groupby(['As_Of_Date', 'ETF']).agg({\n",
    "        'Daily_Rebate_100_Util': 'sum',\n",
    "        'Adjusted_Daily_Rebate_100_Util': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    final_aggregated = pd.merge(\n",
    "        daily_aggregated,\n",
    "        daily_aggregated_100_util,\n",
    "        on=['As_Of_Date', 'ETF'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    output_file = f\"daily_rebate_{etf_name}.csv\"\n",
    "    final_aggregated.to_csv(output_file, index=False)\n",
    "\n",
    "    merged_output_file = f\"merged_data_{etf_name}.csv\"\n",
    "    merged_data.to_csv(merged_output_file, index=False)\n",
    "    print(f\"Data for ETF {etf_name} saved to {output_file}\")\n",
    "    print(f\"Merged data for ETF {etf_name} saved to {merged_output_file}\")\n",
    "\n",
    "# Specify the file paths\n",
    "etf_file = 'etfg_RALS.csv'\n",
    "rebate_data_file = 'kkmnljfpapt1m46e.csv'\n",
    "ffr_data_file = 'DFF.csv'\n",
    "\n",
    "# Run the analysis\n",
    "analyze_missing_utilization(etf_file, rebate_data_file, ffr_data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b0d55-df91-4d3b-becc-25a8b7cd71f0",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3d4b8-8572-4ff9-82b3-919ef03f679d",
   "metadata": {},
   "source": [
    "### Test 2:\n",
    "\n",
    "Similarly, the algorithm cacluates the rebates through mergers and conversions. This time however we are able to use multiple ETF data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d32701-585d-468c-a7cc-62adac87ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_missing_tickers(etf_data):\n",
    "    missing_tickers = etf_data[etf_data['Constituent_Ticker'].isna()]\n",
    "    for index, row in missing_tickers.iterrows():\n",
    "        matching_rows = etf_data[(etf_data['Constituent_Name'] == row['Constituent_Name']) & (etf_data['Constituent_Ticker'].notna())]\n",
    "        if not matching_rows.empty:\n",
    "            etf_data.at[index, 'Constituent_Ticker'] = matching_rows.iloc[0]['Constituent_Ticker']\n",
    "    return etf_data\n",
    "\n",
    "def analyze_missing_utilization(etf_files, rebate_data_file, ffr_data_file):\n",
    "    # Read and process rebate data\n",
    "    rebate_data = pd.read_csv(rebate_data_file, parse_dates=['DataDate'], low_memory=False)\n",
    "    rebate_data['BB_TICKER'] = rebate_data['BB_TICKER'].str.upper().str.strip()\n",
    "    rebate_data['ISIN'] = rebate_data['ISIN'].str.upper().str.strip()\n",
    "    rebate_data['SEDOL'] = rebate_data['SEDOL'].str.upper().str.strip()\n",
    "    rebate_data['InstrumentName'] = rebate_data['InstrumentName'].str.upper().str.strip()\n",
    "\n",
    "    ffr_data = pd.read_csv(ffr_data_file, parse_dates=['DATE'], low_memory=False)\n",
    "    ffr_data.rename(columns={'DATE': 'DataDate', 'DFF': 'FedFundsRate'}, inplace=True)\n",
    "    ffr_data['FedFundsRate'] = ffr_data['FedFundsRate'].astype(float) * 100 / 365  # Converts FFR to bps per day\n",
    "\n",
    "    rebate_data = rebate_data.merge(ffr_data, on='DataDate', how='left')\n",
    "    \n",
    "    rebate_data['IndicativeRebate'] = rebate_data['IndicativeRebate'].abs() * 10000 / 360   #takes absolute value and covnert to BPS  \n",
    "\n",
    "    for etf_file in etf_files:\n",
    "        etf_data = pd.read_csv(etf_file, parse_dates=['As_Of_Date'], low_memory=False)\n",
    "        etf_data['Constituent_Ticker'] = etf_data['Constituent_Ticker'].str.upper().str.strip()\n",
    "        etf_data['ISIN'] = etf_data['ISIN'].str.upper().str.strip()\n",
    "        etf_data['SEDOL'] = etf_data['SEDOL'].str.upper().str.strip()\n",
    "        etf_data['Constituent_Name'] = etf_data['Constituent_Name'].str.upper().str.strip()\n",
    "\n",
    "        etf_data = fill_missing_tickers(etf_data)\n",
    "        \n",
    "        etf_name = etf_file.split('_')[-1].split('.')[0]\n",
    "        etf_data['ETF'] = etf_name\n",
    "\n",
    "        keys_to_try = [\n",
    "            ('Constituent_Ticker', 'BB_TICKER'),\n",
    "            ('Constituent_Name', 'InstrumentName'),\n",
    "            ('SEDOL', 'SEDOL'),\n",
    "            ('ISIN', 'ISIN')\n",
    "        ]\n",
    "\n",
    "        merged_data = pd.DataFrame()\n",
    "        for etf_key, rb_key in keys_to_try:\n",
    "            if etf_data[etf_key].isnull().all() or rebate_data[rb_key].isnull().all():\n",
    "                continue  \n",
    "            \n",
    "            temp_merged = etf_data.merge(rebate_data, left_on=['As_Of_Date', etf_key], right_on=['DataDate', rb_key], how='left', indicator=True)\n",
    "            \n",
    "            valid_rows = temp_merged[(pd.notnull(temp_merged[etf_key])) & (pd.notnull(temp_merged[rb_key]))]\n",
    "            \n",
    "            if not valid_rows.empty:\n",
    "                if merged_data.empty:\n",
    "                    merged_data = valid_rows.copy()\n",
    "                else:\n",
    "                    merged_data = pd.concat([merged_data, valid_rows])\n",
    "\n",
    "        if merged_data.empty:\n",
    "            continue\n",
    "\n",
    "        # Ensure 'InstrumentName' is a string before filtering\n",
    "        merged_data['InstrumentName'] = merged_data['InstrumentName'].astype(str)\n",
    "\n",
    "        # Exclude bonds by filtering out rows with '%' in InstrumentName\n",
    "        merged_data = merged_data[~merged_data['InstrumentName'].str.contains('%')]\n",
    "\n",
    "        # Drop duplicate rows for the same ETF-stock-day combination, keeping the first row only\n",
    "        merged_data = merged_data.drop_duplicates(subset=['As_Of_Date', 'Constituent_Ticker'], keep='first')\n",
    "\n",
    "        # Count missing utilization values\n",
    "        missing_util_count = merged_data['Utilisation'].isna().sum()\n",
    "        print(f\"ETF: {etf_name}, Missing Utilisation Count: {missing_util_count}\")\n",
    "\n",
    "        # Calculate Daily Rebate assuming 100% Utilisation\n",
    "        merged_data['Daily_Rebate_100_Util'] = merged_data['Weight'] * merged_data['IndicativeRebate']\n",
    "\n",
    "        # Calculate Adjusted Daily Rebate assuming 100% Utilisation\n",
    "        merged_data['Adjusted_Daily_Rebate_100_Util'] = merged_data['Weight'] * (merged_data['IndicativeRebate'] + merged_data['FedFundsRate'])\n",
    "\n",
    "        # Handle missing Utilisation by imputing with 0\n",
    "        merged_data['Utilisation'] = merged_data['Utilisation'] / 100\n",
    "\n",
    "        # Calculate Daily Rebate with actual Utilisation\n",
    "        merged_data['Daily_Rebate'] = merged_data['Weight'] * merged_data['IndicativeRebate'] * merged_data['Utilisation']\n",
    "\n",
    "        # Calculate Adjusted Daily Rebate with actual Utilisation\n",
    "        merged_data['Adjusted_Daily_Rebate'] = merged_data['Weight'] * (merged_data['IndicativeRebate'] + merged_data['FedFundsRate']) * merged_data['Utilisation']\n",
    "\n",
    "        daily_aggregated = merged_data.groupby(['As_Of_Date', 'ETF']).agg({\n",
    "            'Daily_Rebate': 'sum', \n",
    "            'Adjusted_Daily_Rebate': 'sum'\n",
    "        }).reset_index()\n",
    "\n",
    "        daily_aggregated_100_util = merged_data.groupby(['As_Of_Date', 'ETF']).agg({\n",
    "            'Daily_Rebate_100_Util': 'sum', \n",
    "            'Adjusted_Daily_Rebate_100_Util': 'sum'\n",
    "        }).reset_index()\n",
    "\n",
    "        final_aggregated = pd.merge(\n",
    "            daily_aggregated, \n",
    "            daily_aggregated_100_util, \n",
    "            on=['As_Of_Date', 'ETF'], \n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        output_file = f\"daily_rebate_{etf_name}.csv\"\n",
    "        final_aggregated.to_csv(output_file, index=False)\n",
    "\n",
    "        merged_output_file = f\"merged_data_{etf_name}.csv\"\n",
    "        merged_data.to_csv(merged_output_file, index=False)\n",
    "        print(f\"Data for ETF {etf_name} saved to {output_file}\")\n",
    "        print(f\"Merged data for ETF {etf_name} saved to {merged_output_file}\")\n",
    "\n",
    "etf_files = ['etfg_SPY.csv', 'etfg_VNQ.csv']\n",
    "rebate_data_file = 'kkmnljfpapt1m46e.csv'\n",
    "ffr_data_file = 'DFF.csv'\n",
    "\n",
    "analyze_missing_utilization(etf_files, rebate_data_file, ffr_data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bb80e-16f6-4f1f-ad7f-05c90bb7e829",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a2ce5-2d9c-4dff-9713-05495e6daf97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
